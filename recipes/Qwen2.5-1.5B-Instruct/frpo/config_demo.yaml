# Model arguments
model_name_or_path: Qwen/Qwen2.5-1.5B-Instruct
model_revision: main
torch_dtype: bfloat16
attn_implementation: flash_attention_2

# Data training arguments
dataset_name: DigitalLearningGmbH/MATH-lighteval
system_prompt: "You are an AI Assistant specialized in generating accurate and runnable code based on user requests.\n\nYour response must follow this structure precisely:\n\n1.  **Internal Thinking (`>Thinking...>` block):** Before generating the code, outline your plan concisely within a `<think>` block. This should include:\n    *   Identifying the programming language (if not specified, make a reasonable choice).\n    *   Listing the core logic steps or functions needed.\n    *   Identifying necessary libraries/modules to import.\n\n2.  **Code Output (`<answer>` block):** Present *only* the complete, runnable code in the `<answer>` block.\n    *   The `<answer>` block must contain *exclusively* code.\n    *   Do *not* include any introductory sentences, explanations, or summaries before or after the code within the `<answer>` block.\n    *   Ensure all necessary imports, dependencies, function/class definitions, and setup are included so the code can be executed directly (copy-paste runnable).\n\n**Example Interaction:**\n\n**User Request:** Write a Python function that takes a list of numbers and returns the sum.\n\n**AI's Expected Response:**\n\n<think>\n1. Language: Python\n2. Logic: Define a function `sum_list` that takes one argument (a list). Use the built-in `sum()` function. Return the result.\n3. Imports: None needed for basic sum."
# GRPO trainer config
beta: 0.01
bf16: true
use_vllm: true
do_eval: false
gradient_accumulation_steps: 4
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
hub_model_id: Qwen2.5-1.5B-Open-R1-Code-GRPO
hub_strategy: every_save
learning_rate: 5.0e-06
log_completions: true
log_level: info
logging_first_step: true
logging_steps: 1
logging_strategy: steps
lr_scheduler_type: cosine_with_min_lr
lr_scheduler_kwargs:
  min_lr_rate: 0.1
max_prompt_length: 1024
max_completion_length: 4096
max_steps: 500
num_generations: 8
num_train_epochs: 1
output_dir: data/Qwen2.5-1.5B-Open-R1-Code-GRPO
overwrite_output_dir: true
per_device_train_batch_size: 16
push_to_hub: true
scale_rewards: false
epsilon: 99
epsilon_high: 9999
report_to:
- wandb
reward_funcs:
- code
- repetition_penalty
reward_weights:
- 1.0
- 0.1
save_strategy: "steps"
save_steps: 50
save_total_limit: 1
seed: 42
temperature: 1.0
warmup_ratio: 0.03
#FRPO独有参数
explore_lambda: 1
explore_mu: 1
exploit_scale: 0.9
schedule_eta: 0.001
schedule_t0: 1000
advantage_epsilon: 1e-8
